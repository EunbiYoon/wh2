[
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "load_training_set",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "load_test_set",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "log_func",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def log_func(num):\n    return np.log10(num)\n######## likelihood_common ########       \n# for train dataset\ndef word_count_set(data_set):\n    # single test document => just list\n    all_words = [word for sublist in data_set for word in sublist]\n    word_counts = Counter(all_words)\n    # each count\n    freq_dataset=pd.DataFrame(word_counts.items(),columns=[\"word\",\"freq_train\"])",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "word_count_set",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def word_count_set(data_set):\n    # single test document => just list\n    all_words = [word for sublist in data_set for word in sublist]\n    word_counts = Counter(all_words)\n    # each count\n    freq_dataset=pd.DataFrame(word_counts.items(),columns=[\"word\",\"freq_train\"])\n    freq_dataset.set_index([\"word\"], drop=True, inplace=True)\n    return freq_dataset\n######## likelihood_standard ######## \n# for test dataset",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "word_count_list",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def word_count_list(data_list):\n    # single test document => just list\n    all_words = [word for word in data_list]\n    word_counts = Counter(all_words)\n    # each count\n    freq_dataset=pd.DataFrame(word_counts.items(),columns=[\"word\",\"freq_test\"])\n    freq_dataset.set_index([\"word\"], drop=True, inplace=True)\n    return freq_dataset\n######## posterior_standard, posterior_laplace ########     \n# Compare probability",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "compare_probability",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def compare_probability(posterior_pos, posterior_neg):\n    if posterior_pos > posterior_neg:\n        result = \"Positive\"\n        print(result)\n    elif posterior_pos < posterior_neg:\n        result = \"Negative\"\n        print(result)\n    elif posterior_pos == posterior_neg:\n        print(\"RANDOM~~~\")\n        result = random.choice([\"Positive\", \"Negative\"])",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "multiply_standard",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def multiply_standard(likelihood_sorted):  \n    # copy the original version\n    likelihood_sorted = likelihood_sorted.copy()\n    # before multiply, make portion : NaN => 0\n    likelihood_sorted.loc[:, \"portion_train_pos\"] = likelihood_sorted[\"portion_train_pos\"].fillna(0)\n    likelihood_sorted.loc[:, \"portion_train_neg\"] = likelihood_sorted[\"portion_train_neg\"].fillna(0)\n    # (portion_train_pos) ^ (freq_test)\n    likelihood_sorted.loc[:, \"multiply_pos\"] = likelihood_sorted[\"portion_train_pos\"] ** likelihood_sorted[\"freq_test\"]\n    likelihood_sorted.loc[:, \"multiply_neg\"] = likelihood_sorted[\"portion_train_neg\"] ** likelihood_sorted[\"freq_test\"]\n    # likely_pos, likely_neg = multiply all the result",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "sum_laplace",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def sum_laplace(likelihood_alpha):  \n    # copy the original version\n    likelihood_alpha = likelihood_alpha.copy()\n    # (portion_train_pos) * log(freq_test)\n    likelihood_alpha.loc[:, \"sum_pos\"] = likelihood_alpha[\"freq_test\"] * log_func(likelihood_alpha[\"portion_train_pos_alpha\"])\n    likelihood_alpha.loc[:, \"sum_neg\"] = likelihood_alpha[\"freq_test\"] * log_func(likelihood_alpha[\"portion_train_neg_alpha\"])\n    # likely_pos, likely_neg = multiply all the result\n    likely_pos=likelihood_alpha[\"sum_pos\"].sum()\n    likely_neg=likelihood_alpha[\"sum_neg\"].sum()\n    return likely_pos, likely_neg",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "draw_table",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def draw_table(confusion_table, accuracy, precision, recall, image_name):\n    fig, ax = plt.subplots(figsize=(6.5, 2.2))  # Create figure and axes\n    ax.axis('off')  # Hide axes\n    # Display the table\n    table = ax.table(cellText=confusion_table.values, colLabels=confusion_table.columns, rowLabels=confusion_table.index, cellLoc='center',  loc='center')\n    table.auto_set_column_width(13)\n    # Adjust table scale for better visibility\n    table.scale(1.5, 1.5)\n    # Set font size for table\n    table.auto_set_font_size(False)  ",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "draw_graph",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def draw_graph(alpha_list, accuracy_list):\n    log_alpha_list = log_func(alpha_list) \n    print(log_alpha_list)\n    plt.figure(figsize=(8, 6))\n    plt.plot(log_alpha_list, accuracy_list, marker='o', linestyle='-')\n    plt.xlabel('alpha (log scale)')\n    plt.ylabel('accuracy')\n    plt.title(\"Figure 2-Graph. Plot of the model's accuracy\")\n    plt.legend()\n    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "preprocessing_data",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def preprocessing_data(train_percen_pos, train_percen_neg, test_percen_pos, test_percen_neg):\n    percentage_positive_instances_train = train_percen_pos\n    percentage_negative_instances_train = train_percen_neg\n    percentage_positive_instances_test = test_percen_pos\n    percentage_negative_instances_test = test_percen_neg\n    (train_pos, train_neg, vocab) = load_training_set(percentage_positive_instances_train, percentage_negative_instances_train)\n    (test_pos, test_neg) = load_test_set(percentage_positive_instances_test, percentage_negative_instances_test)\n    print(\"\\nTrain and Test dataset are completed to preprocessing data\")\n    return train_pos, train_neg, vocab, test_pos, test_neg\n##################################################################",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "prior_standard",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def prior_standard(pos_data_len, neg_data_len):\n    total_len = pos_data_len + neg_data_len\n    return pos_data_len / total_len, neg_data_len / total_len\n# Laplace : Q2 ~ Q6\ndef prior_laplace(pos_data_len, neg_data_len):\n    pos_prior, neg_prior = prior_standard(pos_data_len, neg_data_len)\n    return log_func(pos_prior), log_func(neg_prior)\n##################################################################\n########### 3. Likelihood ###########\n# likelihood_common => contain test and train data freq_pos, freq_neg",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "prior_laplace",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def prior_laplace(pos_data_len, neg_data_len):\n    pos_prior, neg_prior = prior_standard(pos_data_len, neg_data_len)\n    return log_func(pos_prior), log_func(neg_prior)\n##################################################################\n########### 3. Likelihood ###########\n# likelihood_common => contain test and train data freq_pos, freq_neg\ndef likelihood_common(train_pos, train_neg):\n    ###### train ######\n    # input the frequecy of train data\n    freq_train_pos=word_count_set(train_pos)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "likelihood_common",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def likelihood_common(train_pos, train_neg):\n    ###### train ######\n    # input the frequecy of train data\n    freq_train_pos=word_count_set(train_pos)\n    freq_train_neg=word_count_set(train_neg)\n    # merge \n    likelihood_table = pd.concat([freq_train_pos, freq_train_neg],axis=1)\n    likelihood_table.columns=[\"freq_train_pos\",\"freq_train_neg\"]\n    # get the total frequency in the table\n    # use only train data -> Total_Frequency, so it will not affect to out of vocabulary, since vocab consist of train ",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "likelihood_standard",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def likelihood_standard(likelihood_table, test_data, vocab):\n    # Calculate portions\n    pos_sum = likelihood_table.at[\"Total\", \"freq_train_pos\"] # positive -> V\n    neg_sum = likelihood_table.at[\"Total\", \"freq_train_neg\"] # negative -> V\n    likelihood_table = likelihood_table.copy()  # avoid error that return myself\n    likelihood_table[\"portion_train_pos\"] = likelihood_table[\"freq_train_pos\"] / pos_sum\n    likelihood_table[\"portion_train_neg\"] = likelihood_table[\"freq_train_neg\"] / neg_sum\n    # input the frequecy of test data\n    freq_test=word_count_list(test_data)\n    # merge ",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "likelihood_laplace",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def likelihood_laplace(likelihood_table, test_data, alpha, vocab):\n    # Calculate portions\n    pos_sum = likelihood_table.at[\"Total\", \"freq_train_pos\"] \n    neg_sum = likelihood_table.at[\"Total\", \"freq_train_neg\"]\n    # input the frequecy of test data\n    freq_test=word_count_list(test_data)\n    # merge \n    likelihood_test = pd.concat([freq_test, likelihood_table], axis=1)\n    # sort not \"nan\" values from \"freq_test\"\n    likelihood_test = likelihood_test[likelihood_test['freq_test'].notna()].sort_values(by='freq_test')",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "posterior_standard",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def posterior_standard(prior_pos, prior_neg, likely_pos, likely_neg):\n    # test_pos \n    posterior_pos = prior_pos * likely_pos\n    posterior_neg = prior_neg * likely_neg\n    print(f\"prior * likely | pos: {posterior_pos} / neg: {posterior_neg}\")\n    posterior_result=compare_probability(posterior_pos, posterior_neg)\n    print(f\"posterior_result : {posterior_result}\")\n    return posterior_result\n# Laplace Smoothing log(prior probability) + sum (log(likelihoodal))\ndef posterior_laplace(likelihood_table, test_data, vocab, alpha):",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "posterior_laplace",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def posterior_laplace(likelihood_table, test_data, vocab, alpha):\n    # test_pos \n    posterior_pos = prior_pos + likely_pos\n    posterior_neg = prior_neg + likely_neg\n    print(f\"prior * likely | pos: {posterior_pos} / neg: {posterior_neg}\")\n    posterior_result=compare_probability(posterior_pos, posterior_neg)\n    print(f\"posterior_result : {posterior_result}\")\n    return posterior_result\n##################################################################\n########### 5. Confusion Metrix ###########",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def confusion_matrix(posterior_pos, posterior_neg, file_name):\n    print(\"\\nMaking confusion matrix to get accuracy, precision, and recall\")\n    TP=posterior_pos.count(\"Positive\")\n    TN=posterior_neg.count(\"Negative\")\n    FP=posterior_neg.count(\"Positive\")\n    FN=posterior_pos.count(\"Negative\")\n    print(f\"TP:{TP} / FN:{FN} / TN:{TN} / FP:{FP}\")\n    # calcualte accuracy\n    accuracy=(TP+TN)/(TP+TN+FP+FN)\n    # calcualte precision",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "def preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    text = REPLACE_NO_SPACE.sub(\"\", text)\n    text = REPLACE_WITH_SPACE.sub(\" \", text)\n    text = re.sub(r'\\d+', '', text)\n    text = text.lower()\n    words = text.split()\n    return [w for w in words if w not in stop_words]\ndef load_training_set(percentage_positives, percentage_negatives):\n    vocab = set()",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "load_training_set",
        "kind": 2,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "def load_training_set(percentage_positives, percentage_negatives):\n    vocab = set()\n    positive_instances = []\n    negative_instances = []\n    df = pd.read_csv('train-positive.csv')\n    for _, contents in df.iterrows():\n        contents = contents['reviewText']\n        if random.random() > percentage_positives:\n            continue\n        contents = preprocess_text(contents)",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "load_test_set",
        "kind": 2,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "def load_test_set(percentage_positives, percentage_negatives):\n    positive_instances = []\n    negative_instances = []\n    df = pd.read_csv('test-positive.csv')\n    for _, contents in df.iterrows():\n        contents = contents['reviewText']\n        if random.random() > percentage_positives:\n            continue\n        contents = preprocess_text(contents)\n        positive_instances.append(contents)",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "REPLACE_NO_SPACE",
        "kind": 5,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "REPLACE_NO_SPACE = re.compile(\"[._;:!*`Â¦\\'?,\\\"()\\[\\]]\")\nREPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n## remove this part for mac -> add for window\n#nltk.download('stopwords')\ndef preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    text = REPLACE_NO_SPACE.sub(\"\", text)\n    text = REPLACE_WITH_SPACE.sub(\" \", text)\n    text = re.sub(r'\\d+', '', text)\n    text = text.lower()",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "REPLACE_WITH_SPACE",
        "kind": 5,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n## remove this part for mac -> add for window\n#nltk.download('stopwords')\ndef preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    text = REPLACE_NO_SPACE.sub(\"\", text)\n    text = REPLACE_WITH_SPACE.sub(\" \", text)\n    text = re.sub(r'\\d+', '', text)\n    text = text.lower()\n    words = text.split()",
        "detail": "utils",
        "documentation": {}
    }
]